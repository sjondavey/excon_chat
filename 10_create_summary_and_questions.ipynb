{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import src.valid_index\n",
    "importlib.reload(src.valid_index)\n",
    "from src.valid_index import get_excon_manual_index\n",
    "\n",
    "import src.file_tools\n",
    "importlib.reload(src.file_tools)\n",
    "from src.file_tools import read_processed_regs_into_dataframe, get_regulation_detail, num_tokens_from_string\n",
    "\n",
    "import src.embeddings\n",
    "importlib.reload(src.embeddings)\n",
    "from src.embeddings import get_ada_embedding\n",
    "\n",
    "import src.tree_tools\n",
    "importlib.reload(src.tree_tools)\n",
    "from src.tree_tools import build_tree_for_regulation, split_tree\n",
    "\n",
    "import src.summarise_and_question\n",
    "importlib.reload(src.summarise_and_question)\n",
    "from src.summarise_and_question import get_summary_and_questions_for\n",
    "\n",
    "\n",
    "file_list = []\n",
    "file_list.append('./manual/adla_manual.txt')\n",
    "non_text_labels = ['Table', 'Formula', 'Example', 'Definition']\n",
    "\n",
    "index_adla = get_excon_manual_index()\n",
    "df_adla, non_text = read_processed_regs_into_dataframe(file_list=file_list, valid_index_checker=index_adla, non_text_labels=non_text_labels)\n",
    "# Save the processed manual for chat\n",
    "# df_adla.to_csv(\"./inputs/adla_manual.csv\", encoding=\"utf-8\", sep=\"|\", index = False)\n",
    "tree_adla = build_tree_for_regulation(\"ADLA\", df_adla, valid_index_checker=index_adla)\n",
    "\n",
    "section_summary_with_embeddings = \"./tmp/summary_excon_with_embedding.parquet\"\n",
    "section_questions_with_embeddings = \"./tmp/summary_excon_questions_with_embedding.parquet\"\n",
    "headings_index_file = \"./tmp/headings.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the currency split of the tree so you can continue generating summaries and questions\n",
      "Total number of sections: 92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "first_time = False\n",
    "sectioned_df = pd.DataFrame([],columns = [\"section\", \"text\", \"token_count\"])\n",
    "save_sectioned_df_to_file = \"./tmp/adla_manual.csv\"\n",
    "if first_time:\n",
    "    print(\"Loading the initial split of the tree. You will need to make changes to this as you see the data\")\n",
    "    # Starting at an particular parent node (can be the tree root or any child), this method splits up the \n",
    "    # branch into sections where the text does not exceed a certain word_count cap.\n",
    "    sectioned_df = split_tree(tree_adla.root, df_adla, 1000, index_adla)\n",
    "else:\n",
    "    print(\"Loading the currency split of the tree so you can continue generating summaries and questions\")\n",
    "    sectioned_df = pd.read_csv(save_sectioned_df_to_file, encoding=\"utf-8\", sep=\"|\")\n",
    "\n",
    "print(f'Total number of sections: {len(sectioned_df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or load the DataFrames that will hold the text index. Later we will add the embeddings to the same DataFrames. When we do so, loading the embeddings is slow from certain file formats like csv so we just start with a fast loading file format - parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data contains 92 lines of text\n",
      " -- of which there are 5 lines that do not contain index text (e.g. sections with only definitions or indexes)\n",
      "Questions data contains 92 lines of text\n",
      " -- of which there are 5 lines that do not contain index text (e.g. sections with only definitions or indexes)\n",
      "There are a total number of 92 sections to index\n",
      "You have created 100.00 percent of your text index\n"
     ]
    }
   ],
   "source": [
    "df_summary = None\n",
    "if os.path.exists(section_summary_with_embeddings):\n",
    "    df_summary = pd.read_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "    print(f\"Summary data contains {len(df_summary)} lines of text\")\n",
    "    missing = len(df_summary[df_summary[\"text\"] == \"\"])\n",
    "    if missing > 0:\n",
    "        print(f\" -- of which there are {missing} lines that do not contain index text (e.g. sections with only definitions or indexes)\")\n",
    "else:\n",
    "    print(\"Creating a new summary DataFrame\")\n",
    "    df_summary = pd.DataFrame([], columns = [\"text\", \"section\"])\n",
    "\n",
    "\n",
    "df_questions = None\n",
    "if os.path.exists(section_questions_with_embeddings):\n",
    "    df_questions = pd.read_parquet(section_questions_with_embeddings, engine='pyarrow')\n",
    "    print(f\"Questions data contains {len(df_questions)} lines of text\")    \n",
    "    missing = len(df_questions[df_questions[\"text\"] == \"\"])\n",
    "    if missing > 0:\n",
    "        print(f\" -- of which there are {missing} lines that do not contain index text (e.g. sections with only definitions or indexes)\")\n",
    "else:\n",
    "    print(\"Creating a new questions DataFrame\")\n",
    "    df_questions = pd.DataFrame([], columns = [\"text\", \"section\"])\n",
    "\n",
    "index = None\n",
    "if len(df_summary) != len(df_questions):\n",
    "    print(\"The summary and the questions DataFrames do not have the same length\")\n",
    "else:\n",
    "    index = len(df_summary)\n",
    "    p = (index / len(sectioned_df)) * 100\n",
    "    print(f'There are a total number of {len(sectioned_df)} sections to index')\n",
    "    print(f\"You have created {p:.2f} percent of your text index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the text index \n",
    "\n",
    "This is a manual process. We call OpenAI and print out the answers in a format that is used to update the index text but this does need to be edited before it is added to the index so this is not automated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "C.1 FinSurv Reporting System\n",
      "    (J) Systems governance\n",
      "        (i) Inspection manual\n",
      "        The minimum information that should be contained in an inspection manual includes:\n",
      "            (a) a comprehensive flow diagram clearly depicting the flow of transactions through various systems (on-boarding, transactional, accounting and FinSurv Reporting System, including the Reconciliation Module) from capturing to submission of the transactions to the Financial Surveillance Department;\n",
      "            (b) an up to date list of definitions, contact details of the dedicated person(s) responsible for the reporting to Financial Surveillance Department, error handling and the Reconciliation Module; and\n",
      "            (c) suitable back-up procedures (i.e. how often, where, when, by whom, the duration of storage that should be minimum five years and recovery testing). Refer to the inspection manual specimen which is available from the website: www.resbank.co.za by following the links: Home>Financial Surveillance>Authorised Dealers>FinSurv Reporting System>FinSurv Reporting System documents>System Governance Templates or Home>Financial Surveillance>ADLAs>FinSurv Reporting System >FinSurv Reporting System documents>System Governance Templates.\n",
      "        (ii) Pre and post certification managerial letter of comfort\n",
      "            (a) Pre certification managerial letter of comfort must be submitted to the Financial Surveillance Department prior to the inspectors conducting a systems certification of the on-boarding, transactional, accounting and FinSurv Reporting System, including the Reconciliation Module. The pre certification managerial letter of comfort provides assurance that the reporting entity's FinSurv Reporting System complies with the Financial Surveillance Department's reporting requirements. In addition, risks and controls around the systems are properly mitigated to ensure that correct, accurate and comprehensive data is submitted as well as that the respective system is ready for deployment into the production environment.\n",
      "            (b) Post certification managerial letter of comfort must be submitted to the Financial Surveillance Department after the on-boarding, transactional, accounting and FinSurv Reporting System, including the Reconciliation Module have been deployed in a live environment successfully for at least one month.\n",
      "            (c) These pre and post managerial letters of comfort should be completed by the dedicated person responsible for regulatory compliance in consultation with various governance structures within the reporting entity. Refer to the pre and post certification managerial letter of comfort specimens that are available as outlined in (i)(c) above.\n",
      "        (iii) Annual managerial letter of comfort\n",
      "            (a) The reporting entity must submit an annual managerial letter of comfort indicating that it is comfortable with the governance structures and functionality of the FinSurv Reporting System. The letter must be submitted to the Financial Surveillance Department annually within three months after the financial year end of the reporting entity.\n",
      "            (b) The dedicated person responsible for regulatory compliance must be accountable for completing the annual managerial letter of comfort and obtaining confirmation from assurance providers that the independent assurance review of the on-boarding, transactional, accounting and FinSurv Reporting System, including the Reconciliation Module will form part of its ongoing reviews. In this regard, the nature of the assurance procedure must be outlined. Refer to the annual managerial letter of comfort specimen that is available as outlined in (i)(c) above.\n",
      "##############\n",
      "df_summary.loc[index, \"section\"] = \"C.1(J)\"\n",
      "df_summary.loc[index, \"text\"] = \"Governance requirements for FinSurv Reporting System Include preparation of an inspection manual, submission of pre and post certification managerial letters of comfort, and an annual managerial letter of comfort, all addressed to the Financial Surveillance Department. Key elements covered are description of system transaction flows, contact information of personnel responsible for reporting, and backup procedures. The managerial letters of comfort give assurance of system compliance, risk mitigation, data accuracy, readiness for deployment, and the functionality of governance structures. The responsibility for these submissions primarily lies with the dedicated person responsible for regulatory compliance.\"\n",
      "\n",
      "df_questions.loc[index, \"section\"] = \"C.1(J)\"\n",
      "df_questions.loc[index, \"text\"] = \"What is the role of the dedicated person for regulatory compliance in the FinSurv Reporting System?|What are the key elements covered in the governance requirements for the FinSurv Reporting System?|What are the managerial letters of comfort and what do they include?\"\n"
     ]
    }
   ],
   "source": [
    "#model = \"gpt-3.5-turbo\"\n",
    "model=\"gpt-4\"\n",
    "\n",
    "reg_text = sectioned_df.loc[index]['text']\n",
    "print(\"##############\")\n",
    "print(reg_text)\n",
    "print(\"##############\")\n",
    "\n",
    "model_summary, model_questions = get_summary_and_questions_for(reg_text, model = model)\n",
    "\n",
    "#format output\n",
    "section = sectioned_df.loc[index]['section']\n",
    "print(f'df_summary.loc[index, \"section\"] = \"{section}\"')\n",
    "print(f'df_summary.loc[index, \"text\"] = \"{model_summary}\"')\n",
    "print()\n",
    "print(f'df_questions.loc[index, \"section\"] = \"{section}\"')\n",
    "print(f'df_questions.loc[index, \"text\"] = \"{model_questions}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     next_section \u001b[38;5;241m=\u001b[39m \u001b[43msectioned_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sectioned_df[sectioned_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m next_section]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuston, we have a problem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNext section is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index by location index with a non-integer key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1713\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1714\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[0;32m   1716\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_ixs(key, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1646\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1647\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Once the last four lines are manually checked, the edited result is copied here and the summary and question index is updated\n",
    "\n",
    "df_summary.loc[index, \"section\"] = \"C.1(J)\"\n",
    "df_summary.loc[index, \"text\"] = \"Governance requirements for FinSurv Reporting System Include preparation of an inspection manual, submission of pre and post certification managerial letters of comfort, and an annual managerial letter of comfort, all addressed to the Financial Surveillance Department. Key elements covered are description of system transaction flows, contact information of personnel responsible for reporting, and backup procedures. The managerial letters of comfort give assurance of system compliance, risk mitigation, data accuracy, readiness for deployment, and the functionality of governance structures. The responsibility for these submissions primarily lies with the dedicated person responsible for regulatory compliance.\"\n",
    "\n",
    "df_questions.loc[index, \"section\"] = \"C.1(J)\"\n",
    "df_questions.loc[index, \"text\"] = \"What is the role of the dedicated person for regulatory compliance in the FinSurv Reporting System?|What are the key elements covered in the governance requirements for the FinSurv Reporting System?\"\n",
    "\n",
    "\n",
    "index = index + 1\n",
    "if index == len(sectioned_df):\n",
    "    print(\"All done!\")\n",
    "else:\n",
    "    next_section = sectioned_df.iloc[index][\"section\"]\n",
    "    assert len(sectioned_df[sectioned_df[\"section\"] == next_section]) == 1, \"Huston, we have a problem\"\n",
    "    print(f'Next section is {next_section} which is on line {index}')\n",
    "    p = ((index-1) / len(sectioned_df)) * 100\n",
    "    print(f\"You have completed {p:.2f} percent of your work\")\n",
    "    reg_text = sectioned_df.loc[index]['text']\n",
    "    print(\"Next section\")\n",
    "    print(\"##############\")\n",
    "    print(reg_text)\n",
    "    print(\"##############\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes there are errors in the previous code block. We need to be careful when saving over any work we have already done so the \n",
    "# save step is a manual one which needs to be run regularly but without overwriting good data with bad data\n",
    "df_summary.to_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "df_questions.to_parquet(section_questions_with_embeddings, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing how the sections are chunked\n",
    "\n",
    "From time to time we will see instances whe the initial chunk size needs to be adjusted and nodes need to be expanded or collapsed. We do this in two stages. First we can experiment with a new \"token_limit_per_chunk\" for the node in question and once we find the chunking solution we are after, we remove the old chunks and replace them with the new chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will remove 9 row(s) from sectioned_df. From 82 to 91\n",
      "... and will replace them with 1 row(s)\n",
      "     section                                               text  token_count\n",
      "0  C.1(D)(v)  C.1 FinSurv Reporting System\\n    (D) Offshori...         1208\n"
     ]
    }
   ],
   "source": [
    "node_str = \"C.1(D)(v)\"\n",
    "# get the list of indicies that start with this string\n",
    "index_list = sectioned_df.index[sectioned_df['section'].str.startswith(node_str)].tolist()\n",
    "is_consecutive = all(x+1 == y for x, y in zip(index_list[:-1], index_list[1:]))\n",
    "assert is_consecutive, \"The list of indicies that start with the node string is not consecutive so the rest of the logic here will not hold\"\n",
    "start_index_to_replace = index_list[0]\n",
    "end_index_to_replace = index_list[-1] + 1\n",
    "print(f\"This will remove {len(index_list)} row(s) from sectioned_df. From {start_index_to_replace} to {end_index_to_replace}\")\n",
    "\n",
    "# Get the new set of indicies assuming a different chunking length\n",
    "node = tree_adla.get_node(node_str)\n",
    "token_limit_per_chunk = 1300\n",
    "\n",
    "tmp_df = split_tree(node, df_adla, token_limit_per_chunk, index_adla)\n",
    "\n",
    "print(f\"... and will replace them with {len(tmp_df)} row(s)\")\n",
    "print(tmp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the node and all its children with the new DataFrame with a different word_count limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rows(original_df, updated_section_df, start_row, end_row):\n",
    "    before = original_df.iloc[:start_row]\n",
    "    after = original_df.iloc[end_row:]\n",
    "    new_df = pd.concat([before, updated_section_df, after]).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "print(f\"The original data consisted of {sectioned_df} chunks\")\n",
    "sectioned_df = replace_rows(sectioned_df, tmp_df, start_row=start_index_to_replace, end_row=end_index_to_replace)\n",
    "print(f\"Post the update, the data consists of {sectioned_df} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesus saves! \n",
    "But only if he is happy with the results. Check first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectioned_df.to_csv(save_sectioned_df_to_file, encoding=\"utf-8\", sep=\"|\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def count_leading_spaces(s):\n",
    "    match = re.match(r'^\\s*', s)  # Matches leading whitespace\n",
    "    return len(match.group(0))\n",
    "\n",
    "add_embeddings = False\n",
    "ad = False # False = adla data\n",
    "if ad:\n",
    "    definitions_to_process = '#Definition 1'\n",
    "    if definitions_to_process == '#Definition 1':\n",
    "        excon_definitions_and_embeddings_file = \"./tmp/ad_definitions_with_embeddings.csv\"\n",
    "        exclude_first_line = True\n",
    "        start_line = 1\n",
    "    elif definitions_to_process == '#Definition 2':\n",
    "        excon_definitions_and_embeddings_file = \"./tmp/ad_insurance_definitions_with_embeddings.csv\"\n",
    "        exclude_first_line = False\n",
    "        start_line = 0\n",
    "    elif definitions_to_process == '#Definition 3':\n",
    "        excon_definitions_and_embeddings_file = \"./tmp/ad_securities_definitions_with_embeddings.csv\"\n",
    "        exclude_first_line = True\n",
    "        start_line = 1\n",
    "    else:\n",
    "        raise NotImplemented(\"Only implemented for excon Definitions 1, 2 and 3\")\n",
    "else:\n",
    "    definitions_to_process = '#Definition 2'\n",
    "    if definitions_to_process == '#Definition 1':\n",
    "        excon_definitions_and_embeddings_file = \"./tmp/adla_definitions.csv\"\n",
    "        exclude_first_line = True\n",
    "        start_line = 1\n",
    "    elif definitions_to_process == '#Definition 2':\n",
    "        excon_definitions_and_embeddings_file = \"./tmp/adla_cloud_definitions.csv\"\n",
    "        exclude_first_line = False\n",
    "        start_line = 0\n",
    "    # elif definitions_to_process == '#Definition 3':\n",
    "    #     excon_definitions_and_embeddings_file = \"./tmp/excon_securities_definitions_with_embeddings.csv\"\n",
    "    #     exclude_first_line = True\n",
    "    #     start_line = 1\n",
    "    else:\n",
    "        raise NotImplemented(\"Only implemented for excon Definitions 1 and 2\")\n",
    "\n",
    "excon_manual_definitions = []\n",
    "#raw_list = non_text['Definition']['#Definition 1']\n",
    "raw_list = non_text['Definition'][definitions_to_process]\n",
    "number_of_spaces = count_leading_spaces(raw_list[start_line])\n",
    "if number_of_spaces % 4 != 0:\n",
    "    raise ValueError(f\"This line does not have an indent which is a multiple of 4: {raw_list[start_line]}\")\n",
    "\n",
    "current_line = raw_list[start_line]\n",
    "\n",
    "current_line_number_of_spaces = count_leading_spaces(current_line)\n",
    "if current_line_number_of_spaces != number_of_spaces:\n",
    "    print(f\"current_line_number_of_spaces: {current_line_number_of_spaces}\")\n",
    "    print(f'number_of_spaces: {number_of_spaces}')\n",
    "    raise ValueError(f\"This line does not have the correct indentation: {current_line}\")\n",
    "\n",
    "processing_table = False\n",
    "for line_number in range(start_line,len(raw_list)-1):\n",
    "    next_line = raw_list[line_number + 1]\n",
    "    next_line_number_of_spaces = count_leading_spaces(next_line)\n",
    "    if next_line_number_of_spaces % 4 != 0:\n",
    "        raise ValueError(f\"This line does not have an indent which is a multiple of 4: {next_line_number_of_spaces}\")\n",
    "\n",
    "    if current_line_number_of_spaces == next_line_number_of_spaces:\n",
    "        current_line = current_line.lstrip()\n",
    "        if \"|\" in current_line: # processing something with table formatting\n",
    "            processing_table = True\n",
    "            split_line = [x.strip() for x in current_line.split(\"|\")]\n",
    "            excon_manual_definitions.append(split_line)\n",
    "        else:\n",
    "            excon_manual_definitions.append(current_line)\n",
    "        current_line = next_line\n",
    "    else:\n",
    "        current_line = current_line + \"\\n\" + next_line\n",
    "\n",
    "# add the last entry\n",
    "current_line = current_line.lstrip()\n",
    "if processing_table:\n",
    "    split_line = [x.strip() for x in current_line.split(\"|\")]\n",
    "    excon_manual_definitions.append(split_line)\n",
    "else:\n",
    "    excon_manual_definitions.append(current_line.lstrip())\n",
    "\n",
    "if processing_table:\n",
    "    headings = excon_manual_definitions[0]\n",
    "    excon_manual_definitions.pop(0)\n",
    "    df_excon_definitions = pd.DataFrame(excon_manual_definitions, columns=headings) \n",
    "else:\n",
    "    df_excon_definitions = pd.DataFrame(excon_manual_definitions, columns=[\"Definition\"])\n",
    "\n",
    "if add_embeddings:\n",
    "    df_excon_definitions[\"Embedding\"] =df_excon_definitions[\"Definition\"].apply(get_ada_embedding)\n",
    "\n",
    "df_excon_definitions.to_csv(excon_definitions_and_embeddings_file, sep=\"|\", encoding='utf-8', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section headings are also a good index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Indent</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Text</th>\n",
       "      <th>Document</th>\n",
       "      <th>Page</th>\n",
       "      <th>Heading</th>\n",
       "      <th>full_reference</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Legal context</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>Legal context</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>A.2</td>\n",
       "      <td>Authorised entities</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>A.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>(A)</td>\n",
       "      <td>Authorised Dealers in foreign exchange with li...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>A.2(A)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>(B)</td>\n",
       "      <td>Authorised Dealers</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>A.2(B)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>2</td>\n",
       "      <td>(i)</td>\n",
       "      <td>Service related payments</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>C.1(H)(i)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>2</td>\n",
       "      <td>(ii)</td>\n",
       "      <td>Transactions relating to income</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>C.1(H)(ii)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>2</td>\n",
       "      <td>(iii)</td>\n",
       "      <td>Transfers of a current nature</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>C.1(H)(iii)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>1</td>\n",
       "      <td>(I)</td>\n",
       "      <td>Reconciliation module</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>C.1(I)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1</td>\n",
       "      <td>(J)</td>\n",
       "      <td>Systems governance</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>C.1(J)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Indent      Reference                                               Text  \\\n",
       "0         0  Legal context                                                      \n",
       "5         0   Introduction                                                      \n",
       "15        0            A.2                                Authorised entities   \n",
       "16        1            (A)  Authorised Dealers in foreign exchange with li...   \n",
       "18        1            (B)                                 Authorised Dealers   \n",
       "..      ...            ...                                                ...   \n",
       "678       2            (i)                           Service related payments   \n",
       "698       2           (ii)                    Transactions relating to income   \n",
       "704       2          (iii)                      Transfers of a current nature   \n",
       "713       1            (I)                              Reconciliation module   \n",
       "735       1            (J)                                 Systems governance   \n",
       "\n",
       "    Document Page  Heading full_reference  word_count  \n",
       "0                     True  Legal context           0  \n",
       "5                     True   Introduction           0  \n",
       "15                    True            A.2           2  \n",
       "16                    True         A.2(A)           8  \n",
       "18                    True         A.2(B)           2  \n",
       "..       ...  ...      ...            ...         ...  \n",
       "678                   True      C.1(H)(i)           3  \n",
       "698                   True     C.1(H)(ii)           4  \n",
       "704                   True    C.1(H)(iii)           5  \n",
       "713                   True         C.1(I)           2  \n",
       "735                   True         C.1(J)           2  \n",
       "\n",
       "[102 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adla[df_adla[\"Heading\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 is to remove all text that is not tagged as a heading leaving only the index and headings\n",
    "toc_file = \"./tmp/section_numbers_and_headings.txt\" # Note this is a temporary file and will be deleted in a few cells time\n",
    "df = df_adla\n",
    "index = index_adla\n",
    "\n",
    "\n",
    "written_references = set() # only write each reference once\n",
    "with open(toc_file, 'w', encoding = 'utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        if row['full_reference'] not in written_references:\n",
    "            written_references.add(row['full_reference'])\n",
    "            s = ' ' * row['Indent'] * 4 + row['Reference']\n",
    "            # skip the text for into and legal\n",
    "            if row['Heading'] and (row['Indent'] == 0 and row['Text'].strip() in index.exclusion_list):\n",
    "                s = s\n",
    "            elif row['Heading']:               \n",
    "                s += \" \" + row['Text']\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2) Remove all lines that do not have text. Since the only text is for the headers, this removes everything that is not a header.\n",
    "#         Note however that the remaining \"headers\" will not contain the (#headers) markdown so will be treated as text\n",
    "import src.file_tools\n",
    "importlib.reload(src.file_tools)\n",
    "from src.file_tools import process_regulations\n",
    "\n",
    "excon_headers = './tmp/non_empty_headings_excon.txt'  # Note this is a temporary file and will be deleted in a few cells time\n",
    "files_as_list = []\n",
    "files_as_list.append(toc_file)\n",
    "df_toc, non_text_toc = process_regulations(files_as_list, valid_index_checker=index_adla, non_text_labels=non_text_labels)\n",
    "\n",
    "for index, row in df_toc.iterrows():\n",
    "    if row['Text'] != \"\":\n",
    "        df_toc.at[index, 'Heading'] = True\n",
    "\n",
    "tree_toc = build_tree_for_regulation(\"adla_toc\", df_toc, valid_index_checker=index_adla)\n",
    "l = tree_toc._list_node_children(tree_toc.root)\n",
    "with open(excon_headers, 'w', encoding = 'utf-8') as f:\n",
    "    f.write(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3) The file that contains the headers (now as text because they are missing the (#heading) markdown) and load it up\n",
    "#         as if it were the regs themselves\n",
    "files_as_list = []\n",
    "files_as_list.append(excon_headers)\n",
    "df_non_empty_toc, non_text_toc = process_regulations(files_as_list, valid_index_checker=index_adla, non_text_labels=non_text_labels)\n",
    "for index, row in df_non_empty_toc.iterrows():\n",
    "    if row['Text'] != \"\":\n",
    "        df_non_empty_toc.at[index, 'Heading'] = True\n",
    "\n",
    "non_empty_tree_toc = build_tree_for_regulation(\"Excon\", df_non_empty_toc, valid_index_checker=index_adla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excon Manual contains 84 section headings\n"
     ]
    }
   ],
   "source": [
    "# Construct the full reference of each heading and use these as a key for a dictionary where the heading is the value\n",
    "import os\n",
    "\n",
    "def get_leaf_headings(root):\n",
    "    leaf_headings = {}\n",
    "\n",
    "    def recurse(node, heading):\n",
    "        # Add \". \" only if it's not the root node and the node's heading_text is not empty\n",
    "        new_heading = heading + (\". \" + node.heading_text if heading and node.heading_text else node.heading_text)\n",
    "        if not node.children:  # This is a leaf node.\n",
    "            leaf_headings[node.full_node_name] = new_heading\n",
    "        else:  # This is not a leaf node. We continue the recursion.\n",
    "            for child in node.children:\n",
    "                recurse(child, new_heading)\n",
    "\n",
    "    recurse(root, '')  # We start the recursion from the root, with an empty heading.\n",
    "    return leaf_headings\n",
    "\n",
    "leaf_headings = get_leaf_headings(non_empty_tree_toc.root)\n",
    "df_section_headings = pd.DataFrame(list(leaf_headings.items()), columns=['section', 'text'])\n",
    "print(f'Excon Manual contains {len(leaf_headings)} section headings')\n",
    "\n",
    "df_section_headings.to_csv(headings_index_file, encoding = \"utf-8\", sep = \"|\", index = False)\n",
    "if os.path.exists(toc_file):\n",
    "    os.remove(toc_file)\n",
    "if os.path.exists(excon_headers):\n",
    "    os.remove(excon_headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the three indexes and the definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Definitions\n",
    "df_definitions = pd.read_csv(\"./tmp/adla_definitions.csv\", encoding=\"utf-8\", sep=\"|\")\n",
    "df_definitions[\"source\"] = \"all\"\n",
    "df_dfn_tmp = pd.read_csv(\"./tmp/adla_cloud_definitions.csv\", encoding=\"utf-8\", sep=\"|\")\n",
    "df_dfn_tmp.drop(\"Concept\", axis=1, inplace=True)\n",
    "df_dfn_tmp.rename(columns={\"Description\": \"Definition\"}, inplace=True)\n",
    "df_dfn_tmp[\"source\"] = \"cloud\"\n",
    "df_definitions = pd.concat([df_definitions, df_dfn_tmp], ignore_index = True)\n",
    "df_definitions['Embedding'] = df_definitions['Definition'].apply(get_ada_embedding)\n",
    "df_definitions.to_parquet(\"./inputs/adla_definitions.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m df_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_index, df_tmp, df_tmp_2], ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m df_index \u001b[38;5;241m=\u001b[39m df_index[df_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m df_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_ada_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m df_index\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./inputs/adla_index.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\src\\embeddings.py:9\u001b[0m, in \u001b[0;36mget_ada_embedding\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_ada_embedding\u001b[39m(text, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m    \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m \u001b[39m=\u001b[39;49m [text], model\u001b[39m=\u001b[39;49mmodel)[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32me:\\Code\\pdf\\excon\\env\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)"
     ]
    }
   ],
   "source": [
    "# Index\n",
    "\n",
    "df_index = pd.read_parquet(section_questions_with_embeddings, engine='pyarrow')\n",
    "df_index = df_index[df_index[\"text\"] != \"\"] # remove rows that have 'text' == \"\"\n",
    "# the 'text' column for the questions may contain multiple questions separated by a \"|\". The next line expands these rows\n",
    "# so the value in 'text' only contains one question\n",
    "df_index = df_index.drop(\"text\", axis=1).join(df_index[\"text\"].str.split(\"|\", expand=True).stack().reset_index(level=1, drop=True).rename(\"text\"))\n",
    "df_index.reset_index(drop=True, inplace=True)\n",
    "df_index[\"source\"] = \"question\"\n",
    "\n",
    "df_tmp = pd.read_parquet(section_summary_with_embeddings, engine='pyarrow')\n",
    "df_tmp[\"source\"] = \"summary\"\n",
    "\n",
    "df_tmp_2 = pd.read_csv(headings_index_file, encoding = \"utf-8\", sep = \"|\")\n",
    "df_tmp_2[\"source\"] = \"heading\"\n",
    "\n",
    "df_index = pd.concat([df_index, df_tmp, df_tmp_2], ignore_index = True)\n",
    "df_index = df_index[df_index[\"text\"]!= \"\"]\n",
    "df_index = df_index[df_index[\"text\"].notna()] # Remove any NaN's\n",
    "df_index.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_index['Embedding'] = df_index['text'].apply(get_ada_embedding)\n",
    "df_index.to_parquet(\"./inputs/adla_index.parquet\", engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 250 lines\n",
      "Completed 260 lines\n",
      "Completed 270 lines\n",
      "Completed 280 lines\n",
      "Completed 290 lines\n",
      "Completed 300 lines\n",
      "Completed 310 lines\n",
      "Completed 320 lines\n",
      "Completed 330 lines\n"
     ]
    }
   ],
   "source": [
    "# if there is an error somewhere in the generation of the embedding and you need to find it, this is a hacky way to do that\n",
    "increment = 10\n",
    "for i in range(0, len(df_index), increment):\n",
    "    chunk = df_index.iloc[i:i+increment].copy()\n",
    "    chunk[\"Embedding\"] = chunk[\"text\"].apply(get_ada_embedding)\n",
    "    df_index.loc[chunk.index, \"Embedding\"] = chunk[\"Embedding\"]\n",
    "    print(f\"Completed {i+increment} lines\")\n",
    "\n",
    "\n",
    "#df_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "007219f1a1c0c3993c3211d5a541b1fa109902aadb48cb5499ee55023bf45452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
